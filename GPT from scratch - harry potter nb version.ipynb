{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357f8f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5991293"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('../Harry_Potter_all_books_preprocessed.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82085c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BOY WHO LIVED Mr and Mrs Dursley of number four Privet Drive were proud to say that they were perfectly normal thank you very much .They were the last people youd expect to be involved in anything strange or mysterious because they just didnt hold with such nonsense .Mr Dursley was the director of a firm called Grunnings which made drills .He was a big beefy man with hardly any neck although he did have a very large mustache .Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck which came in very useful as she spent so much of her time craning over garden fences spying on the neighbors .The Dursley s had a small son called Dudley and in their opinion there was no finer boy anywhere .The Dursleys had everything they wanted but they also had a secret and their greatest fear was that somebody would discover it .They didnt think they could bear it if anyone found out about the Potters .Mrs Potter was Mrs Dursleys sister but they hadnt met for several years in f\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3132b09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' !.0123456789?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz~‘•■□'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters = sorted(list(set(text)))\n",
    "\"\".join(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedc78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "950eb531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoder : takes a string/text, and gives out indexes of the characters from our vocab\n",
    "#decoder: reverse of encoding, takes in a list of numbers/indexes, and gives back original string\n",
    "\n",
    "## tradeoff between vocabulary size and sequence/token size\n",
    "import tiktoken\n",
    "encoder = tiktoken.get_encoding('gpt2')\n",
    "encoder.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5e29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {s:i for i,s in enumerate(characters) }\n",
    "dec = {i:s for i,s in enumerate(characters) }\n",
    "encoder = lambda meow : [enc[i] for i in meow]\n",
    "decoder = lambda meow2 : \"\".join([dec[i] for i in meow2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf97ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 44, 54, 62]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder('meow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f7a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([33, 21, 18,  0, 15, 28, 38,  0, 36, 21, 28,  0, 25, 22, 35, 18, 17,  0,\n",
      "        26, 57,  0, 40, 53, 43,  0, 26, 57, 58,  0, 17, 60, 57, 58, 51, 44, 64,\n",
      "         0, 54, 45,  0, 53, 60, 52, 41, 44, 57,  0, 45, 54, 60, 57,  0, 29, 57,\n",
      "        48, 61, 44, 59,  0, 17, 57, 48, 61, 44,  0, 62, 44, 57, 44,  0, 55, 57,\n",
      "        54, 60, 43,  0, 59, 54,  0, 58, 40, 64,  0, 59, 47, 40, 59,  0, 59, 47,\n",
      "        44, 64,  0, 62, 44, 57, 44,  0, 55, 44, 57, 45, 44, 42, 59, 51, 64,  0,\n",
      "        53, 54, 57, 52, 40, 51,  0, 59, 47, 40, 53, 50,  0, 64, 54, 60,  0, 61,\n",
      "        44, 57, 64,  0, 52, 60, 42, 47,  0,  2, 33, 47, 44, 64,  0, 62, 44, 57,\n",
      "        44,  0, 59, 47, 44,  0, 51, 40, 58, 59,  0, 55, 44, 54, 55, 51, 44,  0,\n",
      "        64, 54, 60, 43,  0, 44, 63, 55, 44, 42, 59,  0, 59, 54,  0, 41, 44,  0,\n",
      "        48, 53, 61, 54, 51, 61, 44, 43,  0, 48, 53,  0, 40, 53, 64, 59, 47, 48,\n",
      "        53, 46,  0, 58, 59, 57, 40, 53, 46, 44,  0, 54, 57,  0, 52, 64, 58, 59,\n",
      "        44, 57, 48, 54, 60, 58,  0, 41, 44, 42, 40, 60, 58, 44,  0, 59, 47, 44,\n",
      "        64,  0, 49, 60, 58, 59,  0, 43, 48, 43, 53, 59,  0, 47, 54, 51, 43,  0,\n",
      "        62, 48, 59, 47,  0, 58, 60, 42, 47,  0, 53, 54, 53, 58, 44, 53, 58, 44,\n",
      "         0,  2, 26, 57,  0, 17, 60, 57, 58, 51, 44, 64,  0, 62, 40, 58,  0, 59,\n",
      "        47, 44,  0, 43, 48, 57, 44, 42, 59, 54, 57,  0, 54, 45,  0, 40,  0, 45,\n",
      "        48, 57, 52,  0, 42, 40, 51, 51, 44, 43,  0, 20, 57, 60, 53, 53, 48, 53,\n",
      "        46, 58,  0, 62, 47, 48, 42, 47,  0, 52, 40, 43, 44,  0, 43, 57, 48, 51,\n",
      "        51, 58,  0,  2, 21, 44,  0, 62, 40, 58,  0, 40,  0, 41, 48, 46,  0, 41,\n",
      "        44, 44, 45, 64,  0, 52, 40, 53,  0, 62, 48, 59, 47,  0, 47, 40, 57, 43,\n",
      "        51, 64,  0, 40, 53, 64,  0, 53, 44, 42, 50,  0, 40, 51, 59, 47, 54, 60,\n",
      "        46, 47,  0, 47, 44,  0, 43, 48, 43,  0, 47, 40, 61, 44,  0, 40,  0, 61,\n",
      "        44, 57, 64,  0, 51, 40, 57, 46, 44,  0, 52, 60, 58, 59, 40, 42, 47, 44,\n",
      "         0,  2, 26, 57, 58,  0, 17, 60, 57, 58, 51, 44, 64,  0, 62, 40, 58,  0,\n",
      "        59, 47, 48, 53,  0, 40, 53, 43,  0, 41, 51, 54, 53, 43, 44,  0, 40, 53,\n",
      "        43,  0, 47, 40, 43,  0, 53, 44, 40, 57, 51, 64,  0, 59, 62, 48, 42, 44,\n",
      "         0, 59, 47, 44,  0, 60, 58, 60, 40, 51,  0, 40, 52, 54, 60, 53, 59,  0,\n",
      "        54, 45,  0, 53, 44, 42, 50,  0, 62, 47, 48, 42, 47,  0, 42, 40, 52, 44,\n",
      "         0, 48, 53,  0, 61, 44, 57, 64,  0, 60, 58, 44, 45, 60, 51,  0, 40, 58,\n",
      "         0, 58, 47, 44,  0, 58, 55, 44, 53, 59,  0, 58, 54,  0, 52, 60, 42, 47,\n",
      "         0, 54, 45,  0, 47, 44, 57,  0, 59, 48, 52, 44,  0, 42, 57, 40, 53, 48,\n",
      "        53, 46,  0, 54, 61, 44, 57,  0, 46, 40, 57, 43, 44, 53,  0, 45, 44, 53,\n",
      "        42, 44, 58,  0, 58, 55, 64, 48, 53, 46,  0, 54, 53,  0, 59, 47, 44,  0,\n",
      "        53, 44, 48, 46, 47, 41, 54, 57, 58,  0,  2, 33, 47, 44,  0, 17, 60, 57,\n",
      "        58, 51, 44, 64,  0, 58,  0, 47, 40, 43,  0, 40,  0, 58, 52, 40, 51, 51,\n",
      "         0, 58, 54, 53,  0, 42, 40, 51, 51, 44, 43,  0, 17, 60, 43, 51, 44, 64,\n",
      "         0, 40, 53, 43,  0, 48, 53,  0, 59, 47, 44, 48, 57,  0, 54, 55, 48, 53,\n",
      "        48, 54, 53,  0, 59, 47, 44, 57, 44,  0, 62, 40, 58,  0, 53, 54,  0, 45,\n",
      "        48, 53, 44, 57,  0, 41, 54, 64,  0, 40, 53, 64, 62, 47, 44, 57, 44,  0,\n",
      "         2, 33, 47, 44,  0, 17, 60, 57, 58, 51, 44, 64, 58,  0, 47, 40, 43,  0,\n",
      "        44, 61, 44, 57, 64, 59, 47, 48, 53, 46,  0, 59, 47, 44, 64,  0, 62, 40,\n",
      "        53, 59, 44, 43,  0, 41, 60, 59,  0, 59, 47, 44, 64,  0, 40, 51, 58, 54,\n",
      "         0, 47, 40, 43,  0, 40,  0, 58, 44, 42, 57, 44, 59,  0, 40, 53, 43,  0,\n",
      "        59, 47, 44, 48, 57,  0, 46, 57, 44, 40, 59, 44, 58, 59,  0, 45, 44, 40,\n",
      "        57,  0, 62, 40, 58,  0, 59, 47, 40, 59,  0, 58, 54, 52, 44, 41, 54, 43,\n",
      "        64,  0, 62, 54, 60, 51, 43,  0, 43, 48, 58, 42, 54, 61, 44, 57,  0, 48,\n",
      "        59,  0,  2, 33, 47, 44, 64,  0, 43, 48, 43, 53, 59,  0, 59, 47, 48, 53,\n",
      "        50,  0, 59, 47, 44, 64,  0, 42, 54, 60, 51, 43,  0, 41, 44, 40, 57,  0,\n",
      "        48, 59,  0, 48, 45,  0, 40, 53, 64, 54, 53, 44,  0, 45, 54, 60, 53, 43,\n",
      "         0, 54, 60, 59,  0, 40, 41, 54, 60, 59,  0, 59, 47, 44,  0, 29, 54, 59,\n",
      "        59, 44, 57, 58,  0,  2, 26, 57, 58,  0, 29, 54, 59, 59, 44, 57,  0, 62,\n",
      "        40, 58,  0, 26, 57, 58,  0, 17, 60, 57, 58, 51, 44, 64, 58,  0, 58, 48,\n",
      "        58, 59, 44, 57,  0, 41, 60, 59,  0, 59, 47, 44, 64,  0, 47, 40, 43, 53,\n",
      "        59,  0, 52, 44, 59,  0, 45, 54, 57,  0, 58, 44, 61, 44, 57, 40, 51,  0,\n",
      "        64, 44, 40, 57, 58,  0, 48, 53,  0, 45])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encoder(text),dtype=torch.long)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5648b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "672f7341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([24, 8])\n",
      "tensor([[64, 54, 60,  0, 50, 53, 54, 62],\n",
      "        [55, 40, 57, 50,  0,  2, 28, 53],\n",
      "        [59, 44, 57, 41, 44, 44, 57,  0],\n",
      "        [44, 40, 42, 47, 44, 57, 58,  0],\n",
      "        [48, 58,  0, 40, 58,  0, 48, 59],\n",
      "        [40, 43,  0, 53, 44, 61, 44, 57],\n",
      "        [31, 54, 53,  0, 40, 53, 43,  0],\n",
      "        [62, 48, 59, 47,  0, 59, 47, 44],\n",
      "        [ 0, 59, 47, 40, 53,  0, 40, 53],\n",
      "        [64,  0, 40, 53, 64, 52, 54, 57],\n",
      "        [ 0, 62, 48, 65, 40, 57, 43, 58],\n",
      "        [60, 51, 43, 53, 59,  0, 47, 40],\n",
      "        [58, 40, 48, 43,  0, 21, 40, 57],\n",
      "        [43,  0, 21, 40, 57, 57, 64,  0],\n",
      "        [54, 60, 58, 44,  0, 13, 21, 40],\n",
      "        [48, 53,  0, 59, 47, 44,  0, 58],\n",
      "        [48, 54, 53, 58,  0, 54, 45,  0],\n",
      "        [51, 40, 48, 53, 51, 64,  0, 59],\n",
      "        [59,  0, 62, 47, 54,  0, 62, 40],\n",
      "        [55, 44, 53, 52, 54, 60, 59, 47],\n",
      "        [46, 60, 58,  0, 47, 40, 58,  0],\n",
      "        [ 0, 58, 44, 44, 52, 44, 43,  0],\n",
      "        [43, 48, 43, 53, 59,  0, 59, 44],\n",
      "        [48, 43, 53, 59,  0, 52, 54, 61]])\n",
      "targets:\n",
      "torch.Size([24, 8])\n",
      "tensor([[54, 60,  0, 50, 53, 54, 62,  0],\n",
      "        [40, 57, 50,  0,  2, 28, 53, 44],\n",
      "        [44, 57, 41, 44, 44, 57,  0, 42],\n",
      "        [40, 42, 47, 44, 57, 58,  0, 43],\n",
      "        [58,  0, 40, 58,  0, 48, 59,  0],\n",
      "        [43,  0, 53, 44, 61, 44, 57,  0],\n",
      "        [54, 53,  0, 40, 53, 43,  0, 21],\n",
      "        [48, 59, 47,  0, 59, 47, 44,  0],\n",
      "        [59, 47, 40, 53,  0, 40, 53, 64],\n",
      "        [ 0, 40, 53, 64, 52, 54, 57, 44],\n",
      "        [62, 48, 65, 40, 57, 43, 58,  0],\n",
      "        [51, 43, 53, 59,  0, 47, 40, 61],\n",
      "        [40, 48, 43,  0, 21, 40, 57, 57],\n",
      "        [ 0, 21, 40, 57, 57, 64,  0, 51],\n",
      "        [60, 58, 44,  0, 13, 21, 40, 57],\n",
      "        [53,  0, 59, 47, 44,  0, 58, 59],\n",
      "        [54, 53, 58,  0, 54, 45,  0, 51],\n",
      "        [40, 48, 53, 51, 64,  0, 59, 54],\n",
      "        [ 0, 62, 47, 54,  0, 62, 40, 58],\n",
      "        [44, 53, 52, 54, 60, 59, 47, 44],\n",
      "        [60, 58,  0, 47, 40, 58,  0, 55],\n",
      "        [58, 44, 44, 52, 44, 43,  0, 58],\n",
      "        [48, 43, 53, 59,  0, 59, 44, 51],\n",
      "        [43, 53, 59,  0, 52, 54, 61, 44]])\n",
      "----\n",
      "when input is [64] the target: 54\n",
      "when input is [64, 54] the target: 60\n",
      "when input is [64, 54, 60] the target: 0\n",
      "when input is [64, 54, 60, 0] the target: 50\n",
      "when input is [64, 54, 60, 0, 50] the target: 53\n",
      "when input is [64, 54, 60, 0, 50, 53] the target: 54\n",
      "when input is [64, 54, 60, 0, 50, 53, 54] the target: 62\n",
      "when input is [64, 54, 60, 0, 50, 53, 54, 62] the target: 0\n",
      "when input is [55] the target: 40\n",
      "when input is [55, 40] the target: 57\n",
      "when input is [55, 40, 57] the target: 50\n",
      "when input is [55, 40, 57, 50] the target: 0\n",
      "when input is [55, 40, 57, 50, 0] the target: 2\n",
      "when input is [55, 40, 57, 50, 0, 2] the target: 28\n",
      "when input is [55, 40, 57, 50, 0, 2, 28] the target: 53\n",
      "when input is [55, 40, 57, 50, 0, 2, 28, 53] the target: 44\n",
      "when input is [59] the target: 44\n",
      "when input is [59, 44] the target: 57\n",
      "when input is [59, 44, 57] the target: 41\n",
      "when input is [59, 44, 57, 41] the target: 44\n",
      "when input is [59, 44, 57, 41, 44] the target: 44\n",
      "when input is [59, 44, 57, 41, 44, 44] the target: 57\n",
      "when input is [59, 44, 57, 41, 44, 44, 57] the target: 0\n",
      "when input is [59, 44, 57, 41, 44, 44, 57, 0] the target: 42\n",
      "when input is [44] the target: 40\n",
      "when input is [44, 40] the target: 42\n",
      "when input is [44, 40, 42] the target: 47\n",
      "when input is [44, 40, 42, 47] the target: 44\n",
      "when input is [44, 40, 42, 47, 44] the target: 57\n",
      "when input is [44, 40, 42, 47, 44, 57] the target: 58\n",
      "when input is [44, 40, 42, 47, 44, 57, 58] the target: 0\n",
      "when input is [44, 40, 42, 47, 44, 57, 58, 0] the target: 43\n",
      "when input is [48] the target: 58\n",
      "when input is [48, 58] the target: 0\n",
      "when input is [48, 58, 0] the target: 40\n",
      "when input is [48, 58, 0, 40] the target: 58\n",
      "when input is [48, 58, 0, 40, 58] the target: 0\n",
      "when input is [48, 58, 0, 40, 58, 0] the target: 48\n",
      "when input is [48, 58, 0, 40, 58, 0, 48] the target: 59\n",
      "when input is [48, 58, 0, 40, 58, 0, 48, 59] the target: 0\n",
      "when input is [40] the target: 43\n",
      "when input is [40, 43] the target: 0\n",
      "when input is [40, 43, 0] the target: 53\n",
      "when input is [40, 43, 0, 53] the target: 44\n",
      "when input is [40, 43, 0, 53, 44] the target: 61\n",
      "when input is [40, 43, 0, 53, 44, 61] the target: 44\n",
      "when input is [40, 43, 0, 53, 44, 61, 44] the target: 57\n",
      "when input is [40, 43, 0, 53, 44, 61, 44, 57] the target: 0\n",
      "when input is [31] the target: 54\n",
      "when input is [31, 54] the target: 53\n",
      "when input is [31, 54, 53] the target: 0\n",
      "when input is [31, 54, 53, 0] the target: 40\n",
      "when input is [31, 54, 53, 0, 40] the target: 53\n",
      "when input is [31, 54, 53, 0, 40, 53] the target: 43\n",
      "when input is [31, 54, 53, 0, 40, 53, 43] the target: 0\n",
      "when input is [31, 54, 53, 0, 40, 53, 43, 0] the target: 21\n",
      "when input is [62] the target: 48\n",
      "when input is [62, 48] the target: 59\n",
      "when input is [62, 48, 59] the target: 47\n",
      "when input is [62, 48, 59, 47] the target: 0\n",
      "when input is [62, 48, 59, 47, 0] the target: 59\n",
      "when input is [62, 48, 59, 47, 0, 59] the target: 47\n",
      "when input is [62, 48, 59, 47, 0, 59, 47] the target: 44\n",
      "when input is [62, 48, 59, 47, 0, 59, 47, 44] the target: 0\n",
      "when input is [0] the target: 59\n",
      "when input is [0, 59] the target: 47\n",
      "when input is [0, 59, 47] the target: 40\n",
      "when input is [0, 59, 47, 40] the target: 53\n",
      "when input is [0, 59, 47, 40, 53] the target: 0\n",
      "when input is [0, 59, 47, 40, 53, 0] the target: 40\n",
      "when input is [0, 59, 47, 40, 53, 0, 40] the target: 53\n",
      "when input is [0, 59, 47, 40, 53, 0, 40, 53] the target: 64\n",
      "when input is [64] the target: 0\n",
      "when input is [64, 0] the target: 40\n",
      "when input is [64, 0, 40] the target: 53\n",
      "when input is [64, 0, 40, 53] the target: 64\n",
      "when input is [64, 0, 40, 53, 64] the target: 52\n",
      "when input is [64, 0, 40, 53, 64, 52] the target: 54\n",
      "when input is [64, 0, 40, 53, 64, 52, 54] the target: 57\n",
      "when input is [64, 0, 40, 53, 64, 52, 54, 57] the target: 44\n",
      "when input is [0] the target: 62\n",
      "when input is [0, 62] the target: 48\n",
      "when input is [0, 62, 48] the target: 65\n",
      "when input is [0, 62, 48, 65] the target: 40\n",
      "when input is [0, 62, 48, 65, 40] the target: 57\n",
      "when input is [0, 62, 48, 65, 40, 57] the target: 43\n",
      "when input is [0, 62, 48, 65, 40, 57, 43] the target: 58\n",
      "when input is [0, 62, 48, 65, 40, 57, 43, 58] the target: 0\n",
      "when input is [60] the target: 51\n",
      "when input is [60, 51] the target: 43\n",
      "when input is [60, 51, 43] the target: 53\n",
      "when input is [60, 51, 43, 53] the target: 59\n",
      "when input is [60, 51, 43, 53, 59] the target: 0\n",
      "when input is [60, 51, 43, 53, 59, 0] the target: 47\n",
      "when input is [60, 51, 43, 53, 59, 0, 47] the target: 40\n",
      "when input is [60, 51, 43, 53, 59, 0, 47, 40] the target: 61\n",
      "when input is [58] the target: 40\n",
      "when input is [58, 40] the target: 48\n",
      "when input is [58, 40, 48] the target: 43\n",
      "when input is [58, 40, 48, 43] the target: 0\n",
      "when input is [58, 40, 48, 43, 0] the target: 21\n",
      "when input is [58, 40, 48, 43, 0, 21] the target: 40\n",
      "when input is [58, 40, 48, 43, 0, 21, 40] the target: 57\n",
      "when input is [58, 40, 48, 43, 0, 21, 40, 57] the target: 57\n",
      "when input is [43] the target: 0\n",
      "when input is [43, 0] the target: 21\n",
      "when input is [43, 0, 21] the target: 40\n",
      "when input is [43, 0, 21, 40] the target: 57\n",
      "when input is [43, 0, 21, 40, 57] the target: 57\n",
      "when input is [43, 0, 21, 40, 57, 57] the target: 64\n",
      "when input is [43, 0, 21, 40, 57, 57, 64] the target: 0\n",
      "when input is [43, 0, 21, 40, 57, 57, 64, 0] the target: 51\n",
      "when input is [54] the target: 60\n",
      "when input is [54, 60] the target: 58\n",
      "when input is [54, 60, 58] the target: 44\n",
      "when input is [54, 60, 58, 44] the target: 0\n",
      "when input is [54, 60, 58, 44, 0] the target: 13\n",
      "when input is [54, 60, 58, 44, 0, 13] the target: 21\n",
      "when input is [54, 60, 58, 44, 0, 13, 21] the target: 40\n",
      "when input is [54, 60, 58, 44, 0, 13, 21, 40] the target: 57\n",
      "when input is [48] the target: 53\n",
      "when input is [48, 53] the target: 0\n",
      "when input is [48, 53, 0] the target: 59\n",
      "when input is [48, 53, 0, 59] the target: 47\n",
      "when input is [48, 53, 0, 59, 47] the target: 44\n",
      "when input is [48, 53, 0, 59, 47, 44] the target: 0\n",
      "when input is [48, 53, 0, 59, 47, 44, 0] the target: 58\n",
      "when input is [48, 53, 0, 59, 47, 44, 0, 58] the target: 59\n",
      "when input is [48] the target: 54\n",
      "when input is [48, 54] the target: 53\n",
      "when input is [48, 54, 53] the target: 58\n",
      "when input is [48, 54, 53, 58] the target: 0\n",
      "when input is [48, 54, 53, 58, 0] the target: 54\n",
      "when input is [48, 54, 53, 58, 0, 54] the target: 45\n",
      "when input is [48, 54, 53, 58, 0, 54, 45] the target: 0\n",
      "when input is [48, 54, 53, 58, 0, 54, 45, 0] the target: 51\n",
      "when input is [51] the target: 40\n",
      "when input is [51, 40] the target: 48\n",
      "when input is [51, 40, 48] the target: 53\n",
      "when input is [51, 40, 48, 53] the target: 51\n",
      "when input is [51, 40, 48, 53, 51] the target: 64\n",
      "when input is [51, 40, 48, 53, 51, 64] the target: 0\n",
      "when input is [51, 40, 48, 53, 51, 64, 0] the target: 59\n",
      "when input is [51, 40, 48, 53, 51, 64, 0, 59] the target: 54\n",
      "when input is [59] the target: 0\n",
      "when input is [59, 0] the target: 62\n",
      "when input is [59, 0, 62] the target: 47\n",
      "when input is [59, 0, 62, 47] the target: 54\n",
      "when input is [59, 0, 62, 47, 54] the target: 0\n",
      "when input is [59, 0, 62, 47, 54, 0] the target: 62\n",
      "when input is [59, 0, 62, 47, 54, 0, 62] the target: 40\n",
      "when input is [59, 0, 62, 47, 54, 0, 62, 40] the target: 58\n",
      "when input is [55] the target: 44\n",
      "when input is [55, 44] the target: 53\n",
      "when input is [55, 44, 53] the target: 52\n",
      "when input is [55, 44, 53, 52] the target: 54\n",
      "when input is [55, 44, 53, 52, 54] the target: 60\n",
      "when input is [55, 44, 53, 52, 54, 60] the target: 59\n",
      "when input is [55, 44, 53, 52, 54, 60, 59] the target: 47\n",
      "when input is [55, 44, 53, 52, 54, 60, 59, 47] the target: 44\n",
      "when input is [46] the target: 60\n",
      "when input is [46, 60] the target: 58\n",
      "when input is [46, 60, 58] the target: 0\n",
      "when input is [46, 60, 58, 0] the target: 47\n",
      "when input is [46, 60, 58, 0, 47] the target: 40\n",
      "when input is [46, 60, 58, 0, 47, 40] the target: 58\n",
      "when input is [46, 60, 58, 0, 47, 40, 58] the target: 0\n",
      "when input is [46, 60, 58, 0, 47, 40, 58, 0] the target: 55\n",
      "when input is [0] the target: 58\n",
      "when input is [0, 58] the target: 44\n",
      "when input is [0, 58, 44] the target: 44\n",
      "when input is [0, 58, 44, 44] the target: 52\n",
      "when input is [0, 58, 44, 44, 52] the target: 44\n",
      "when input is [0, 58, 44, 44, 52, 44] the target: 43\n",
      "when input is [0, 58, 44, 44, 52, 44, 43] the target: 0\n",
      "when input is [0, 58, 44, 44, 52, 44, 43, 0] the target: 58\n",
      "when input is [43] the target: 48\n",
      "when input is [43, 48] the target: 43\n",
      "when input is [43, 48, 43] the target: 53\n",
      "when input is [43, 48, 43, 53] the target: 59\n",
      "when input is [43, 48, 43, 53, 59] the target: 0\n",
      "when input is [43, 48, 43, 53, 59, 0] the target: 59\n",
      "when input is [43, 48, 43, 53, 59, 0, 59] the target: 44\n",
      "when input is [43, 48, 43, 53, 59, 0, 59, 44] the target: 51\n",
      "when input is [48] the target: 43\n",
      "when input is [48, 43] the target: 53\n",
      "when input is [48, 43, 53] the target: 59\n",
      "when input is [48, 43, 53, 59] the target: 0\n",
      "when input is [48, 43, 53, 59, 0] the target: 52\n",
      "when input is [48, 43, 53, 59, 0, 52] the target: 54\n",
      "when input is [48, 43, 53, 59, 0, 52, 54] the target: 61\n",
      "when input is [48, 43, 53, 59, 0, 52, 54, 61] the target: 44\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 24 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a7686c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 71])\n",
      "tensor(4.9205, grad_fn=<NllLossBackward0>)\n",
      " 3yhb■rnCYI1Z‘ZjKrtYdC2dB5iWFK!vIH4VBji6Nes8szMeFzq5ic5■C□55VED?VB9sYz‘OAtayh6Mes8‘Zj4nQj?r2TEi5rIFqB\n"
     ]
    }
   ],
   "source": [
    "## an embedding table is a simple lookup table for word embeddings using indices.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # of size vocab_size * vocab_size\n",
    "        # every value from our input is going to retrieve a row from the embedding table\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C) batch * time * channel / 4 * 8 * 65\n",
    "        # logits are the scores/predictions for the next character\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # 2-D\n",
    "            targets = targets.view(B*T) # 1-D\n",
    "            loss = F.cross_entropy(logits, targets) # neg log likelihood\n",
    "            ## pytorch wants B*C*T\n",
    "            ## expected loss - > ln(1/vocab_size) -> ln(1/65) = 4.17\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            #pluck out last element in time dimension as they are our predictionss\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1) #one element prediction for each batch \n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "idx = torch.zeros((1, 1), dtype=torch.long) # 1 * 1 tensor of zeroth index i.e apostrophe\n",
    "print(decoder(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "# untrained model gives garbage :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e28161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#better optiimzers?\n",
    "\n",
    "# Experiment with SGD + Momentum: If Adam doesn't perform as well as expected, try SGD with momentum. It can sometimes provide better generalization.\n",
    "\n",
    "# RMSprop: If you have non-stationary data or find that the gradients vary significantly, RMSprop can be beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ebd7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e36d8522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4069924354553223\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(5000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step() # update params\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e78f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " warinethi□.Tan am le Whe g t arentt we Yo■n hily thE2.GGre It e y ffoll5ite s lant dl wach ouain wli\n"
     ]
    }
   ],
   "source": [
    "print(decoder(m.generate(idx, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "torch.manual_seed(4)\n",
    "\n",
    "\n",
    "with open('../Harry_Potter_all_books_preprocessed.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "#encoder : takes a string/text, and gives out indexes of the characters from our vocab\n",
    "#decoder: reverse of encoding, takes in a list of numbers/indexes, and gives back original string\n",
    "\n",
    "enc = {s:i for i,s in enumerate(characters) }\n",
    "dec = {i:s for i,s in enumerate(characters) }\n",
    "encoder = lambda meow : [enc[i] for i in meow]\n",
    "decoder = lambda meow2 : \"\".join([dec[i] for i in meow2])\n",
    "\n",
    "data = torch.tensor(encoder(text),dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() ##  Disables gradient computation, which reduces memory usage and speeds up computations during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensures that layers like dropout and batch normalization are in evaluation mode, providing more accurate loss measurements for both training and validation sets.\n",
    "# In contrast, the training loop is focused on parameter updates and does not ensure such an accurate evaluation context.\n",
    "# computations are efficient and do not interfere with the training process via torch.no_grad()\n",
    "# estimate_loss function computes the average loss over multiple mini-batches, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "877e038f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8242, val loss 4.8388\n",
      "step 300: train loss 2.7296, val loss 2.7216\n",
      "step 600: train loss 2.4657, val loss 2.4579\n",
      "step 900: train loss 2.4228, val loss 2.4130\n",
      "step 1200: train loss 2.4004, val loss 2.3908\n",
      "step 1500: train loss 2.4045, val loss 2.3753\n",
      "step 1800: train loss 2.3940, val loss 2.3806\n",
      "step 2100: train loss 2.3915, val loss 2.3691\n",
      "step 2400: train loss 2.3863, val loss 2.3693\n",
      "step 2700: train loss 2.3903, val loss 2.3677\n",
      " g tos hand aryou bes theeato hoy beeryey osnontauly bely y s boust Prs .Intand Nad het Antid o iker u fom mad .Ivear w .Butan id honinor tt harcKOhera us nthadn .If wh herys .Spkng simma bey Harondmbers ncix haord iuns we t orinasclur lfis beld he w nokfis ?Thisupppimay benowhiowagldeinghig gonthe he adSouerint wrick ve hingehad he hin Mreaimme lind penthus hin ioatid y ch s n thond !My t they u lineadorhtevee .He yst homu t tithoon ur fed indgrowous ut Wek walate wimbures frmip chewead wes and \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "with open('../Harry_Potter_all_books_preprocessed.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac82854",
   "metadata": {},
   "outputs": [],
   "source": [
    "## math trick for self attention\n",
    "## for a position,calculate average of all preceding vectors and at that position via channels\n",
    "## information should only flow backwards from the latest token\n",
    "## take in channels info and go backwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0b9334bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9414,  1.2632],\n",
       "         [-0.1838,  0.1505],\n",
       "         [ 0.1075, -0.2780],\n",
       "         [-2.6021,  0.6245],\n",
       "         [-0.8684, -0.2051],\n",
       "         [ 0.3976,  0.6699],\n",
       "         [-0.0537,  0.0467],\n",
       "         [-1.7671, -2.1205]],\n",
       "\n",
       "        [[ 1.5191, -0.6682],\n",
       "         [ 0.0031, -0.1535],\n",
       "         [ 1.1396, -0.2302],\n",
       "         [ 1.1877,  0.7677],\n",
       "         [-0.7588, -0.1853],\n",
       "         [-0.8558, -0.2346],\n",
       "         [-0.4215,  0.8488],\n",
       "         [-0.6776, -0.9445]],\n",
       "\n",
       "        [[-0.4815,  1.2434],\n",
       "         [ 2.3693,  0.2829],\n",
       "         [-0.2345,  1.6892],\n",
       "         [ 0.2716, -0.1365],\n",
       "         [-0.6948, -1.3186],\n",
       "         [-0.9694,  0.6403],\n",
       "         [ 0.8201, -0.9151],\n",
       "         [-2.1437,  1.4072]],\n",
       "\n",
       "        [[-0.0263,  2.7204],\n",
       "         [-0.5955,  0.9871],\n",
       "         [ 1.0861,  0.0610],\n",
       "         [ 0.0417,  0.6783],\n",
       "         [-0.8952, -1.0143],\n",
       "         [-0.2429, -1.5727],\n",
       "         [ 1.3940, -0.1941],\n",
       "         [ 0.0048, -1.3165]]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(4)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a773df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9414,  1.2632]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780],\n",
      "        [-2.6021,  0.6245]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780],\n",
      "        [-2.6021,  0.6245],\n",
      "        [-0.8684, -0.2051]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780],\n",
      "        [-2.6021,  0.6245],\n",
      "        [-0.8684, -0.2051],\n",
      "        [ 0.3976,  0.6699]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780],\n",
      "        [-2.6021,  0.6245],\n",
      "        [-0.8684, -0.2051],\n",
      "        [ 0.3976,  0.6699],\n",
      "        [-0.0537,  0.0467]])\n",
      "tensor([[-0.9414,  1.2632],\n",
      "        [-0.1838,  0.1505],\n",
      "        [ 0.1075, -0.2780],\n",
      "        [-2.6021,  0.6245],\n",
      "        [-0.8684, -0.2051],\n",
      "        [ 0.3976,  0.6699],\n",
      "        [-0.0537,  0.0467],\n",
      "        [-1.7671, -2.1205]])\n",
      "tensor([[ 1.5191, -0.6682]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302],\n",
      "        [ 1.1877,  0.7677]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302],\n",
      "        [ 1.1877,  0.7677],\n",
      "        [-0.7588, -0.1853]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302],\n",
      "        [ 1.1877,  0.7677],\n",
      "        [-0.7588, -0.1853],\n",
      "        [-0.8558, -0.2346]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302],\n",
      "        [ 1.1877,  0.7677],\n",
      "        [-0.7588, -0.1853],\n",
      "        [-0.8558, -0.2346],\n",
      "        [-0.4215,  0.8488]])\n",
      "tensor([[ 1.5191, -0.6682],\n",
      "        [ 0.0031, -0.1535],\n",
      "        [ 1.1396, -0.2302],\n",
      "        [ 1.1877,  0.7677],\n",
      "        [-0.7588, -0.1853],\n",
      "        [-0.8558, -0.2346],\n",
      "        [-0.4215,  0.8488],\n",
      "        [-0.6776, -0.9445]])\n",
      "tensor([[-0.4815,  1.2434]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892],\n",
      "        [ 0.2716, -0.1365]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892],\n",
      "        [ 0.2716, -0.1365],\n",
      "        [-0.6948, -1.3186]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892],\n",
      "        [ 0.2716, -0.1365],\n",
      "        [-0.6948, -1.3186],\n",
      "        [-0.9694,  0.6403]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892],\n",
      "        [ 0.2716, -0.1365],\n",
      "        [-0.6948, -1.3186],\n",
      "        [-0.9694,  0.6403],\n",
      "        [ 0.8201, -0.9151]])\n",
      "tensor([[-0.4815,  1.2434],\n",
      "        [ 2.3693,  0.2829],\n",
      "        [-0.2345,  1.6892],\n",
      "        [ 0.2716, -0.1365],\n",
      "        [-0.6948, -1.3186],\n",
      "        [-0.9694,  0.6403],\n",
      "        [ 0.8201, -0.9151],\n",
      "        [-2.1437,  1.4072]])\n",
      "tensor([[-0.0263,  2.7204]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610],\n",
      "        [ 0.0417,  0.6783]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610],\n",
      "        [ 0.0417,  0.6783],\n",
      "        [-0.8952, -1.0143]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610],\n",
      "        [ 0.0417,  0.6783],\n",
      "        [-0.8952, -1.0143],\n",
      "        [-0.2429, -1.5727]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610],\n",
      "        [ 0.0417,  0.6783],\n",
      "        [-0.8952, -1.0143],\n",
      "        [-0.2429, -1.5727],\n",
      "        [ 1.3940, -0.1941]])\n",
      "tensor([[-0.0263,  2.7204],\n",
      "        [-0.5955,  0.9871],\n",
      "        [ 1.0861,  0.0610],\n",
      "        [ 0.0417,  0.6783],\n",
      "        [-0.8952, -1.0143],\n",
      "        [-0.2429, -1.5727],\n",
      "        [ 1.3940, -0.1941],\n",
      "        [ 0.0048, -1.3165]])\n",
      "tensor([[[-0.9414,  1.2632],\n",
      "         [-0.5626,  0.7069],\n",
      "         [-0.3392,  0.3786],\n",
      "         [-0.9050,  0.4400],\n",
      "         [-0.8976,  0.3110],\n",
      "         [-0.6818,  0.3708],\n",
      "         [-0.5920,  0.3245],\n",
      "         [-0.7389,  0.0189]],\n",
      "\n",
      "        [[ 1.5191, -0.6682],\n",
      "         [ 0.7611, -0.4109],\n",
      "         [ 0.8873, -0.3506],\n",
      "         [ 0.9624, -0.0710],\n",
      "         [ 0.6181, -0.0939],\n",
      "         [ 0.3725, -0.1174],\n",
      "         [ 0.2591,  0.0207],\n",
      "         [ 0.1420, -0.1000]],\n",
      "\n",
      "        [[-0.4815,  1.2434],\n",
      "         [ 0.9439,  0.7632],\n",
      "         [ 0.5511,  1.0719],\n",
      "         [ 0.4812,  0.7698],\n",
      "         [ 0.2460,  0.3521],\n",
      "         [ 0.0435,  0.4001],\n",
      "         [ 0.1544,  0.2122],\n",
      "         [-0.1329,  0.3616]],\n",
      "\n",
      "        [[-0.0263,  2.7204],\n",
      "         [-0.3109,  1.8538],\n",
      "         [ 0.1547,  1.2562],\n",
      "         [ 0.1265,  1.1117],\n",
      "         [-0.0779,  0.6865],\n",
      "         [-0.1054,  0.3100],\n",
      "         [ 0.1088,  0.2380],\n",
      "         [ 0.0958,  0.0437]]])\n"
     ]
    }
   ],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        print(xprev)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n",
    "print(xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21bfc3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9414,  1.2632],\n",
       "        [-0.5626,  0.7069],\n",
       "        [-0.3392,  0.3786],\n",
       "        [-0.9050,  0.4400],\n",
       "        [-0.8976,  0.3110],\n",
       "        [-0.6818,  0.3708],\n",
       "        [-0.5920,  0.3245],\n",
       "        [-0.7389,  0.0189]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cf84a3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.33923333333333333"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.9414-0.1838+0.1075)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a560d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets use mathematical trick of self attention\n",
    "## mulitplying a normalized version of a lower left triangle of ones with our data can give\n",
    "## us the average must easily\n",
    "## batched matrix multiply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "560bba52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9414,  1.2632],\n",
       "        [-0.5626,  0.7069],\n",
       "        [-0.3392,  0.3786],\n",
       "        [-0.9050,  0.4400],\n",
       "        [-0.8976,  0.3110],\n",
       "        [-0.6818,  0.3708],\n",
       "        [-0.5920,  0.3245],\n",
       "        [-0.7389,  0.0189]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "950d573e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## softmax version\n",
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) ## tokens from past cant communicate as we set them to -inf\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n",
    "## weighted aggregation of the past tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## self-attention: get information about tokens that are interesting to oneself\n",
    "## every token will give out a query and a key\n",
    "## the way we form relations / affinities between tokens is by a dot product between queries and keys\n",
    "## that dot product is wei\n",
    "## query -> what i am looking for\n",
    "## key -> what do i contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c96b3d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) ## removin this line allows all tokens to comm with each other\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "79e13c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0][7] \n",
    "## becomes data dependent\n",
    "## 8th token knows what it contents and its position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74fb777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", \n",
    "#the queries still get produced from x,\n",
    "#but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "## \"Scaled\" attention additional divides wei by 1/sqrt(head_size). \n",
    "#This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd966342",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4151fda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9857, -1.7370,  0.4189,  0.2985, -0.5451,  0.4942, -0.7267,\n",
       "          -0.4810],\n",
       "         [-0.6151,  0.7704,  0.1215,  0.1193, -1.0559, -0.1234,  0.3918,\n",
       "          -0.2687],\n",
       "         [ 0.4511,  0.6600,  0.8736, -0.5065,  0.8595,  0.2483,  0.7095,\n",
       "          -0.3241],\n",
       "         [ 1.4350, -0.5599,  1.2163, -0.0813,  1.7313,  0.3421, -0.3146,\n",
       "          -0.9178],\n",
       "         [-2.0204,  1.8716, -1.1214, -0.1317, -0.4320,  0.8461,  1.0991,\n",
       "           1.8651],\n",
       "         [ 1.0000,  0.5394,  0.9807, -0.0900,  0.7364,  1.3018,  1.4779,\n",
       "           1.2385],\n",
       "         [ 1.0542, -0.5249,  0.1258, -0.0781,  0.8236, -1.0546,  0.3601,\n",
       "          -0.5679],\n",
       "         [ 0.2587,  0.1620,  0.6471,  0.2837,  1.2641,  0.3890, -0.6218,\n",
       "          -0.4601]],\n",
       "\n",
       "        [[ 0.2958,  0.3852, -0.7456,  0.0486, -0.1722, -0.3054,  0.8299,\n",
       "           0.4364],\n",
       "         [-0.8550,  0.2635,  1.0761,  0.9544,  0.7529, -0.9505,  0.2712,\n",
       "          -0.7474],\n",
       "         [-0.9295, -0.1556, -0.0649, -0.3967, -0.1137, -1.0016,  0.5251,\n",
       "           2.1469],\n",
       "         [ 0.4260,  0.5476, -0.1825,  0.4262,  0.4614,  0.7707, -0.0319,\n",
       "           0.0419],\n",
       "         [ 0.9852, -1.8435,  0.5392,  0.9052,  0.0147, -0.0083, -0.3682,\n",
       "          -1.4787],\n",
       "         [ 0.0243, -0.7921,  1.1611, -2.1399,  0.8785, -3.7707,  0.2854,\n",
       "           0.2840],\n",
       "         [ 1.2051,  0.7425,  0.1625,  0.1487, -0.3684,  1.1103, -0.6565,\n",
       "          -0.5399],\n",
       "         [ 0.4974, -0.6444, -0.5425,  1.1529,  1.4586,  2.1972, -1.4472,\n",
       "          -2.8702]],\n",
       "\n",
       "        [[ 0.8568,  2.0571, -0.7006,  0.3831,  0.3106, -0.0532, -0.3264,\n",
       "          -1.9622],\n",
       "         [ 0.2715,  1.6624, -1.0427,  0.8949,  0.9935, -1.7049, -0.6244,\n",
       "          -0.0398],\n",
       "         [-0.2850, -1.0568,  1.0730, -0.3639,  1.4113,  0.4204,  0.0832,\n",
       "           1.8698],\n",
       "         [ 0.2770, -0.6132,  0.0788, -0.9589,  0.1094, -0.6141,  0.2004,\n",
       "          -1.6000],\n",
       "         [-0.6044,  0.2721,  2.6773, -0.6017,  0.0375, -1.9897, -0.9305,\n",
       "           0.2045],\n",
       "         [ 0.1326,  1.4459,  0.4910,  0.8230,  0.4905, -0.6744,  0.1504,\n",
       "           0.7052],\n",
       "         [ 0.9300,  0.8989, -0.2437,  1.2802,  1.8367, -1.6352,  1.1763,\n",
       "          -1.7056],\n",
       "         [-0.6255,  0.1929,  0.2220, -1.3950,  1.2366, -0.2456,  1.7097,\n",
       "           0.0183]],\n",
       "\n",
       "        [[ 0.7970, -0.1589,  2.1649,  1.3842,  1.0258, -0.0408, -0.9887,\n",
       "          -2.0557],\n",
       "         [-0.8892,  1.2347, -1.0261,  0.4946, -0.5050,  0.4870,  0.8439,\n",
       "           0.4716],\n",
       "         [-0.3764,  0.7035,  1.2153,  2.7291, -0.8751,  1.2909, -0.0223,\n",
       "          -2.2251],\n",
       "         [-3.0034, -0.0472, -1.8734,  0.4687,  1.1202,  0.0969,  1.7634,\n",
       "           0.6773],\n",
       "         [-0.8947, -0.0660, -1.8381,  0.0621, -0.6480, -0.0997,  0.6156,\n",
       "           0.9758],\n",
       "         [ 0.1174, -0.0431, -0.3131, -1.1377, -0.2750,  0.2928,  0.8689,\n",
       "           1.4676],\n",
       "         [-0.1954,  0.3430,  0.8964, -0.2087,  0.9346, -0.0980,  0.7557,\n",
       "          -0.7743],\n",
       "         [ 2.0831,  0.4608,  4.1820, -0.3133,  1.4190,  1.7620,  0.2617,\n",
       "          -1.2753]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d406d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea479e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## only self atteion with feedforward, no cross attention\n",
    "## a decoder only transformer (triangular mask for attention)\n",
    "## pre tuning of chatgpt: training on a chunk of internet text for the model to spit out the same"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
